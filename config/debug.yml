# This config contains debug settings for training
clear_dir: False
dataset_dir: "dataset/preprocessed/"
ImageEncoder:
    img_size: 224
    patch_size: 16
    in_channels: 3
    embed_dim: 64
    num_layers: 2
    num_heads: 8
    ffn_dim: 256
    dropout: 0.1
    ffn_dropout: 0.2

MAEdecoder:
    img_size: 224
    patch_size: 16
    in_channels: 3
    embed_dim: 64
    num_layers: 1
    num_heads: 8
    ffn_dim: 256
    dropout: 0.1
    ffn_dropout: 0.2

LanguageDecoder:
    img_feature_shape: [196, 64]
    embed_dim: 32
    num_layers: 2
    num_heads: 8
    ffn_dim: 256
    dropout: 0.1
    ffn_dropout: 0.2

DataLoaderTrain:
    batch_size: 16
    add_augmentation: True

DataLoaderVal:
    batch_size: 16
    add_augmentation: False

TrainerMAE:
    lr_start: 1.0e-4
    lr_end: 1.0e-6
    weight_decay: 0.05
    train_num_steps: 40
    grad_clip: 1.0
    sample_every: 10
    save_every: 10
    results_folder: "debug/pretrain"
    use_amp: True
    use_latest_checkpoint: True
    mask_ratio: 0.75

TrainerCaptioning:
    lr_start: 1.5e-4
    lr_end: 1.0e-6
    wd_encoder: 0.05
    wd_decoder: 0.01
    train_num_steps: 25
    warm_up_pct: 0.1
    frozen_enc_pct: 0.3
    grad_clip: 1.0
    sample_every: 5
    save_every: 10
    eval_every: 5
    results_folder: "debug/captioning"
    use_amp: True
    use_latest_checkpoint: 1 # Load in model weights from pre-training
    eps: 0.10
    max_len: 5

TrainerSCST:
    lr_start: 2.0e-5
    lr_end: 1.0e-6
    wd_encoder: 0.05
    wd_decoder: 0.01
    train_num_steps: 25
    warm_up_pct: 0.05
    frozen_enc_pct: 0.0
    grad_clip: 1.0
    sample_every: 5
    save_every: 10
    eval_every: 10
    results_folder: "debug/captioning"
    use_amp: True
    use_latest_checkpoint: 2 # Load in only the model weights
    eps: 0.10
    max_len: 5
    lambda_xe: 0.1
